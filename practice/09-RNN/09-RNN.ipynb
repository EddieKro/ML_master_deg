{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMDB Review Classification Battlefield - Contestants : Feedforward, CNN, RNN, LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we are going to do sentiment classification on a movie review dataset. We are going to build a feedforward net, a convolutional neural net, a recurrent net and combine one or more of them to understand performance of each of them. A sentence can be thought of as a sequence of words which have semantic connections across time. By semantic connection, we mean that the words that occur earlier in the sentence influence the sentence's structure and meaning in the latter part of the sentence. There are also semantic connections backwards in a sentence, in an ideal case (in which we use RNNs from both directions and combine their outputs). But for the purpose of this tutorial, we are going to restrict ourselves to only uni-directional RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small\n",
    "vocabulary_size = 10000\n",
    "\n",
    "#We also want to have a finite length of reviews and not have to process really long sentences.\n",
    "max_review_length = 500\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZATION\n",
    "\n",
    "For practical data science applications, we need to convert text into tokens since the machine understands only numbers and not really English words like humans can. As a simple example of tokenization, we can see a small example.\n",
    "\n",
    "Assume we have 5 sentences. This is how we tokenize them into numbers once we create a dictionary.\n",
    "\n",
    "1. i have books - [1, 4, 7]\n",
    "2. interesting books are useful [10,2,9,8]\n",
    "3. i have computers [1,4,6]\n",
    "4. computers are interesting and useful [6,9,11,10,8]\n",
    "5. books and computers are both valuable. [2,10,2,9,13,12]\n",
    "6. Bye Bye [7,7]\n",
    "\n",
    "Create tokens for vocabulary based on frequency of occurrence. Hence, we assign the following tokens\n",
    "\n",
    "I-1, books-2, computers-3, have-4, are-5, computers-6,bye-7, useful-8, are-9, and-10,interesting-11, valuable-12, both-13\n",
    "\n",
    "Thankfully, in our dataset it is internally handled and each sentence is represented in such tokenized form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb \n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\eddie\\kma_course\\lib\\site-packages\\keras\\datasets\\imdb.py:92: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews 25000\n",
      "Length of first and fifth review before padding 218 147\n",
      "First review [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "First label 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\eddie\\kma_course\\lib\\site-packages\\keras\\datasets\\imdb.py:93: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
    "np.load = np_load_old\n",
    "\n",
    "print('Number of reviews', len(X_train))\n",
    "print('Length of first and fifth review before padding', len(X_train[0]) ,len(X_train[4]))\n",
    "print('First review', X_train[0])\n",
    "print('First label', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data\n",
    "\n",
    "Pad sequences in order to ensure that all inputs have same sentence length and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first and fifth review after padding 500 500\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print('Length of first and fifth review after padding', len(X_train[0]) ,len(X_train[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 1(a) : FEEDFORWARD NETWORKS WITHOUT EMBEDDINGS \n",
    "\n",
    "Let us build a single layer feedforward net with 250 nodes. Each input would be a 500-dim vector of tokens since we padded all our sequences to size 500.\n",
    "\n",
    "<b> EXERCISE </b> : Calculate the number of parameters involved in this network and implement a feedforward net to do classification without looking at cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch import nn\n",
    "# from catalyst import dl\n",
    "batch_size=2048\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "train_set = TensorDataset(torch.FloatTensor(X_train),torch.FloatTensor(y_train).view(25000,1))\n",
    "train_loader = DataLoader(train_set,batch_size=batch_size)#250 examples a time\n",
    "\n",
    "test_set = TensorDataset(torch.FloatTensor(X_test),torch.FloatTensor(y_test).view(25000,1))\n",
    "test_loader = DataLoader(test_set,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self,input_shape=500, hidden_size=250,output_size=1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "# ff_model =FeedForwardNet()\n",
    "# ff_model = ff_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### YOUR CODE HERE ####\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(500,250),\n",
    "#     nn.Linear(250,1),\n",
    "#     nn.Sigmoid()\n",
    "# )\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,epoch,criterion,optimizer,device=device,verbose=0):\n",
    "    model.train()\n",
    "    train_loss, correct = 0,0\n",
    "    for batch_id, (data,target) in enumerate(train_loader):\n",
    "        \n",
    "\n",
    "        data,target = data.to(device),target.to(device)\n",
    "        output = model(data)\n",
    "        optimizer.zero_grad()\n",
    "#         loss = F.binary_cross_entropy(output,target)\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct += output.round().eq(target).sum().item()\n",
    "        train_loss+=loss.item()\n",
    "        if batch_id%100==0 and verbose==1:\n",
    "            print(f\"Training Epoch:{epoch} [{batch_id*len(data)}/{len(train_loader.dataset)}] {100.*batch_id / len(train_loader) :.2f}%\\tLoss: {loss.item():.4f}\")\n",
    "    \n",
    "    train_loss/=len(train_loader.dataset)\n",
    "    acc = correct/len(train_loader.dataset)\n",
    "    \n",
    "    print(f\"\\nTrain: Average loss: {train_loss:.4f}\\t Accuracy: {acc:.4f}\")\n",
    "\n",
    "def validate(model,test_loader, device=device):\n",
    "    model.eval()\n",
    "    test_loss =0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data,target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss +=F.binary_cross_entropy(output, target).item()\n",
    "            correct += output.round().eq(target).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct/ len(test_loader.dataset)\n",
    "    \n",
    "    print(f'Valid: Average loss: {test_loss:.4f}\\t Accuracy: {acc:.8f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(modelType,n_epochs,train_loader,test_loader, optimizer,criterion,lr=1e-2):\n",
    "    model = modelType()\n",
    "    model = model.to(device)\n",
    "    optimizer = optimizer(model.parameters(), lr=lr)\n",
    "    criterion= criterion(reduction='mean')\n",
    "    for epoch in range(n_epochs):\n",
    "        train(model,train_loader,epoch, criterion,optimizer)\n",
    "#         print(next(model.parameters())[0][42])\n",
    "        validate(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: Average loss: 0.0233\t Accuracy: 0.5026\n",
      "Valid: Average loss: 0.0233\t Accuracy: 0.49788000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0233\t Accuracy: 0.5004\n",
      "Valid: Average loss: 0.0233\t Accuracy: 0.49640000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0233\t Accuracy: 0.5006\n",
      "Valid: Average loss: 0.0232\t Accuracy: 0.49940000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0234\t Accuracy: 0.4989\n",
      "Valid: Average loss: 0.0233\t Accuracy: 0.49988000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0235\t Accuracy: 0.4996\n",
      "Valid: Average loss: 0.0233\t Accuracy: 0.49812000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0235\t Accuracy: 0.4991\n",
      "Valid: Average loss: 0.0235\t Accuracy: 0.49696000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0235\t Accuracy: 0.5002\n",
      "Valid: Average loss: 0.0236\t Accuracy: 0.49580000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0237\t Accuracy: 0.5000\n",
      "Valid: Average loss: 0.0238\t Accuracy: 0.49716000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0238\t Accuracy: 0.4996\n",
      "Valid: Average loss: 0.0239\t Accuracy: 0.49716000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0239\t Accuracy: 0.5014\n",
      "Valid: Average loss: 0.0241\t Accuracy: 0.49840000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD\n",
    "criterion = nn.BCELoss\n",
    "training_loop(FeedForwardNet,10, train_loader,test_loader,optimizer,criterion,0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion : Why was the performance bad ? What was wrong with tokenization ? \n",
    "\n",
    "### MODEL 1(b) : FEEDFORWARD NETWORKS WITH EMBEDDINGS\n",
    "\n",
    "#### What is an embedding layer ? \n",
    "\n",
    "An embedding is a linear projection from one vector space to another. We usually use embeddings to project the one-hot encodings of words on to a lower-dimensional continuous space so that the input surface is dense and possibly smooth. According to the model, an embedding layer is just a transformation from $\\mathbb{R}^{inp}$ to $\\mathbb{R}^{emb}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do embedding to dim 100 (in keras, tf, PyTorch: with Embedding layer) and after flattening add a dense layer with 250 units. Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(torch.LongTensor(X_train),torch.FloatTensor(y_train).view(25000,1))\n",
    "train_loader = DataLoader(train_set, batch_size=256)#250 examples a time\n",
    "\n",
    "test_set = TensorDataset(torch.LongTensor(X_test),torch.FloatTensor(y_test).view(25000,1))\n",
    "test_loader = DataLoader(test_set, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FF_Embeddings(nn.Module):\n",
    "    def __init__(self, input_dim=500,embedding_dim=100, num_embeddings=vocabulary_size,hidden_dim=250, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim*input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,output_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).view(x.size(0),-1)\n",
    "        x = self.fc1(embedded)\n",
    "        x = self.fc2(x)\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: Average loss: 0.0036\t Accuracy: 0.5548\n",
      "Valid: Average loss: 0.0033\t Accuracy: 0.55060000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0022\t Accuracy: 0.7280\n",
      "Valid: Average loss: 0.0033\t Accuracy: 0.57544000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0018\t Accuracy: 0.7885\n",
      "Valid: Average loss: 0.0032\t Accuracy: 0.59608000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0019\t Accuracy: 0.7761\n",
      "Valid: Average loss: 0.0029\t Accuracy: 0.62696000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0019\t Accuracy: 0.7954\n",
      "Valid: Average loss: 0.0060\t Accuracy: 0.54012000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0015\t Accuracy: 0.8460\n",
      "Valid: Average loss: 0.0046\t Accuracy: 0.57120000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0012\t Accuracy: 0.8786\n",
      "Valid: Average loss: 0.0037\t Accuracy: 0.61376000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0010\t Accuracy: 0.9051\n",
      "Valid: Average loss: 0.0034\t Accuracy: 0.63828000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0008\t Accuracy: 0.9263\n",
      "Valid: Average loss: 0.0035\t Accuracy: 0.64956000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0007\t Accuracy: 0.9400\n",
      "Valid: Average loss: 0.0036\t Accuracy: 0.65476000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loop(FF_Embeddings,10, train_loader,test_loader,optimizer,criterion,lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2 : CONVOLUTIONAL NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text can be thought of as 1-dimensional sequence and we can apply 1-D Convolutions over a set of words. Let us walk through convolutions on text data with this blog.\n",
    "\n",
    "http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/11/27/Understanding-Convolutions-In-Text/\n",
    "\n",
    "Fit a 1D convolution with 200 filters, kernel size 3 followed by a feedforward layer of 250 nodes and ReLU, sigmoid activations as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "cnn_model = nn.Sequential(\n",
    "    nn.Conv1d(in_channels=200,out_channels=250,kernel_size=3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight [250, 200, 3], but got 2-dimensional input of size [250, 500] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3c6b5c39f5fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\eddie\\kma_course\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eddie\\kma_course\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eddie\\kma_course\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    257\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m    258\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 259\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight [250, 200, 3], but got 2-dimensional input of size [250, 500] instead"
     ]
    }
   ],
   "source": [
    "cnn_model.forward(torch.Tensor(X_train[:250]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 3 : SIMPLE RNN\n",
    "\n",
    "Two of the best blogs that help understand the workings of a RNN and LSTM are\n",
    "\n",
    "1. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "2. http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Mathematically speaking, a simple RNN does the following. It constructs a set of hidden states using the state variable from the previous timestep and the input at current time. Mathematically, a simpleRNN can be defined by the following relation.\n",
    "\n",
    "<center>$h_t = \\sigma(W([h_{t-1},x_{t}])+b)$\n",
    "    \n",
    "If we extend this recurrence relation to the length of sequences we have in hand, we have our RNN network constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do simple RNN (keras, rf: SimpleRNN layer, pytorch: RNN layer) with 100 units with the input from embedding layer. How are the results different from the previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_shape=500, hidden_dim=100, output_shape=1, vocab_size = vocabulary_size, embedding_dim=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=vocabulary_size)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(input_shape * hidden_dim, output_shape)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "    \n",
    "    def forward(self,x):\n",
    "        embedded = self.embedding(x)\n",
    "        output,hidden = self.rnn(embedded)\n",
    "        output,hidden = self.rnn(output)\n",
    "        output = output.view(x.size(0),-1)\n",
    "        x = self.linear(output)\n",
    "        return self.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: Average loss: 0.0027\t Accuracy: 0.5058\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.51516000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0027\t Accuracy: 0.5284\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.51772000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0027\t Accuracy: 0.5422\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.52072000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0027\t Accuracy: 0.5576\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.52516000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0027\t Accuracy: 0.5728\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.52828000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0027\t Accuracy: 0.5861\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.53104000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0026\t Accuracy: 0.6001\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.53316000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0026\t Accuracy: 0.6137\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.53644000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0026\t Accuracy: 0.6255\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.54124000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0026\t Accuracy: 0.6353\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.54368000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "training_loop(RNNModel,10,train_loader,test_loader,optimizer,criterion,lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: Average loss: 0.0027\t Accuracy: 0.5224\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.53724000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0026\t Accuracy: 0.6175\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.57048000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0025\t Accuracy: 0.6785\n",
      "Valid: Average loss: 0.0026\t Accuracy: 0.60260000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0023\t Accuracy: 0.7257\n",
      "Valid: Average loss: 0.0025\t Accuracy: 0.62772000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0022\t Accuracy: 0.7587\n",
      "Valid: Average loss: 0.0024\t Accuracy: 0.65528000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0020\t Accuracy: 0.7842\n",
      "Valid: Average loss: 0.0024\t Accuracy: 0.67448000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0018\t Accuracy: 0.8065\n",
      "Valid: Average loss: 0.0023\t Accuracy: 0.69060000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0017\t Accuracy: 0.8258\n",
      "Valid: Average loss: 0.0022\t Accuracy: 0.70480000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0016\t Accuracy: 0.8461\n",
      "Valid: Average loss: 0.0022\t Accuracy: 0.71516000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0014\t Accuracy: 0.8628\n",
      "Valid: Average loss: 0.0021\t Accuracy: 0.72356000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "training_loop(RNNModel,20,train_loader,test_loader,optimizer,criterion,lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: Average loss: 0.0028\t Accuracy: 0.5152\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.51464000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0026\t Accuracy: 0.5938\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.53548000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0025\t Accuracy: 0.6559\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.55144000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0025\t Accuracy: 0.6966\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.56496000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0024\t Accuracy: 0.7245\n",
      "Valid: Average loss: 0.0026\t Accuracy: 0.57308000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0023\t Accuracy: 0.7422\n",
      "Valid: Average loss: 0.0026\t Accuracy: 0.57976000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0022\t Accuracy: 0.7575\n",
      "Valid: Average loss: 0.0026\t Accuracy: 0.58672000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0022\t Accuracy: 0.7694\n",
      "Valid: Average loss: 0.0026\t Accuracy: 0.59316000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0021\t Accuracy: 0.7794\n",
      "Valid: Average loss: 0.0026\t Accuracy: 0.59928000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0020\t Accuracy: 0.7888\n",
      "Valid: Average loss: 0.0026\t Accuracy: 0.60344000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD\n",
    "training_loop(RNNModel,10,train_loader,test_loader,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNNs and vanishing/exploding gradients\n",
    "\n",
    "Let us use sigmoid activations as example. Derivative of a sigmoid can be written as \n",
    "<center> $\\sigma'(x) = \\sigma(x) \\cdot \\sigma(1-x)$. </center>\n",
    "\n",
    "<img src = \"fig/vanishing_gradients.png\">\n",
    "Remember RNN is a \"really deep\" feedforward network (when unrolled in time). Hence, backpropagation happens from $h_t$ all the way to $h_1$. Also realize that sigmoid gradients are multiplicatively dependent on the value of sigmoid. Hence, if the non-activated output of any layer $h_l$ is < 0, then $\\sigma$ tends to 0, effectively \"vanishing\" gradient. Any layer that the current layer backprops to $H_{1:L-1}$ do not learn anything useful out of the gradients.\n",
    "\n",
    "#### LSTMs and GRU\n",
    "LSTM and GRU are two sophisticated implementations of RNN which essentially are built on what we call as gates. A gate is a probability number between 0 and 1. For instance, LSTM is built on these state updates \n",
    "\n",
    "Note : L is just a linear transformation L(x) = W*x + b.\n",
    "\n",
    "$f_t = \\sigma(L([h_{t-1},x_t))$\n",
    "\n",
    "$i_t = \\sigma(L([h_{t-1},x_t))$\n",
    "\n",
    "$o_t = \\sigma(L([h_{t-1},x_t))$\n",
    "\n",
    "$\\hat{C}_t = \\tanh(L([h_{t-1},x_t))$\n",
    "\n",
    "$C_t = f_t * C_{t-1}+i_t*\\hat{C}_t$  (Using the forget gate, the neural network can learn to control how much information it has to retain or forget)\n",
    "\n",
    "$h_t = o_t * \\tanh(c_t)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 4 : LSTM\n",
    "\n",
    "In the next step, we will implement a LSTM model to do classification. Use the same architecture as before. Try experimenting with increasing the number of nodes, stacking multiple layers, applyong dropouts etc. Check the number of parameters that this model entails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_shape=500, hidden_dim=100, output_shape=1, vocab_size = vocabulary_size, embedding_dim=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=vocabulary_size)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(input_shape * hidden_dim, output_shape)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "    \n",
    "    def forward(self,x):\n",
    "        embedded = self.embedding(x)\n",
    "        output,hidden = self.lstm(embedded)\n",
    "        output,hidden = self.lstm(output)\n",
    "        output = output.view(x.size(0),-1)\n",
    "        x = self.linear(output)\n",
    "        return self.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: Average loss: 0.0028\t Accuracy: 0.5028\n",
      "Valid: Average loss: 0.0027\t Accuracy: 0.54556000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0026\t Accuracy: 0.6177\n",
      "Valid: Average loss: 0.0024\t Accuracy: 0.65872000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0017\t Accuracy: 0.7932\n",
      "Valid: Average loss: 0.0015\t Accuracy: 0.82592000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0012\t Accuracy: 0.8772\n",
      "Valid: Average loss: 0.0014\t Accuracy: 0.84528000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0009\t Accuracy: 0.9028\n",
      "Valid: Average loss: 0.0017\t Accuracy: 0.82176000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0007\t Accuracy: 0.9317\n",
      "Valid: Average loss: 0.0015\t Accuracy: 0.85052000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0006\t Accuracy: 0.9462\n",
      "Valid: Average loss: 0.0018\t Accuracy: 0.83172000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0005\t Accuracy: 0.9540\n",
      "Valid: Average loss: 0.0017\t Accuracy: 0.84144000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0003\t Accuracy: 0.9748\n",
      "Valid: Average loss: 0.0018\t Accuracy: 0.84380000\n",
      "\n",
      "\n",
      "Train: Average loss: 0.0002\t Accuracy: 0.9842\n",
      "Valid: Average loss: 0.0026\t Accuracy: 0.81592000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "training_loop(LSTMModel,10,train_loader,test_loader,optimizer,criterion,lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 5 : CNN + LSTM \n",
    "\n",
    "CNNs are good at learning spatial features and sentences can be thought of as 1-D spatial vectors (dimension being connotated by the sequence ordering among the words in the sentence.). We apply a LSTM over the features learned by the CNN (after a maxpooling layer). This leverages the power of CNNs and LSTMs combined. We expect the CNN to be able to pick out invariant features across the 1-D spatial structure(i.e. sentence) that characterize good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer followed by a feedforward for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION\n",
    "\n",
    "We saw the power of sequence models and how they are useful in text classification. They give a solid performance, low memory footprint (thanks to shared parameters) and are able to understand and leverage the temporally connected information contained in the inputs. There is still an open debate about the performance vs memory benefits of CNNs vs RNNs in the research community."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
